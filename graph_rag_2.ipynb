{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\vishn\\myenv\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\vishn\\myenv\\lib\\site-packages (0.3.5)\n",
      "Requirement already satisfied: langsmith in c:\\users\\vishn\\myenv\\lib\\site-packages (0.1.142)\n",
      "Requirement already satisfied: langgraph in c:\\users\\vishn\\myenv\\lib\\site-packages (0.2.45)\n",
      "Requirement already satisfied: langchainhub in c:\\users\\vishn\\myenv\\lib\\site-packages (0.1.21)\n",
      "Requirement already satisfied: langchain_experimental in c:\\users\\vishn\\myenv\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\vishn\\myenv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: neo4j in c:\\users\\vishn\\myenv\\lib\\site-packages (5.26.0)\n",
      "Requirement already satisfied: pypdf2 in c:\\users\\vishn\\myenv\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\vishn\\myenv\\lib\\site-packages (3.7.5)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain_community) (2.6.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langsmith) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langsmith) (3.10.6)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langgraph) (2.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.32 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langgraph) (0.1.35)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchainhub) (23.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchainhub) (2.32.0.20240712)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vishn\\myenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Using cached thinc-8.3.2-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vishn\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\vishn\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vishn\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vishn\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\vishn\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vishn\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\vishn\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph) (1.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vishn\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vishn\\myenv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached blis-1.0.1-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Using cached thinc-8.3.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "  Using cached thinc-8.3.0-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\vishn\\myenv\\lib\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\vishn\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\vishn\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vishn\\myenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\vishn\\myenv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vishn\\myenv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\vishn\\myenv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vishn\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installing all required dependencies for LangChain, LangSmith \n",
    "try:\n",
    "    %pip install -U langchain langchain_community langsmith langgraph langchainhub langchain_experimental pandas neo4j pypdf2 spacy\n",
    "\n",
    "except Exception as e:  \n",
    "    print(f\"An error occurred during installation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the NEO4J_URI environment variable\n",
    "os.environ['NEO4J_URI'] = \"bolt://localhost:7687\"\n",
    "os.environ['NEO4J_USERNAME'] = 'neo4j'\n",
    "os.environ['NEO4J_PASSWORD'] = 'your_password'\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vishn\\myenv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from langchain.tools import Tool\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ArXiv Research Tool\n",
    "from datetime import datetime\n",
    "\n",
    "class ArxivResearchTool:\n",
    "    def __init__(self, max_results=5):\n",
    "        self.max_results = max_results\n",
    "\n",
    "    def download_paper_pdf(self, pdf_link, save_path):\n",
    "        response = requests.get(pdf_link)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Paper downloaded successfully to {save_path}\")\n",
    "            return f\"Paper downloaded successfully to {save_path}\"\n",
    "        else:\n",
    "            return f\"Failed to download paper (status code: {response.status_code})\"\n",
    "        \n",
    "    def fetch_papers(self, topic: str):\n",
    "        url = f\"http://export.arxiv.org/api/query?search_query=all:{topic}&start=0&max_results={self.max_results}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return f\"Error: Unable to fetch data from ArXiv API (status code: {response.status_code})\"\n",
    "\n",
    "        root = ET.fromstring(response.content)\n",
    "        papers = []\n",
    "\n",
    "        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "            title_text = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "            title = title_text.replace('\\n', '')\n",
    "            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "            link = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
    "            pdf_link = link.replace('/abs/', '/pdf/') + \".pdf\"  # PDF link for full paper\n",
    "            local_pdf_path = os.path.join(\"D:\\Coding\\GraphRAG-with-Llama-3.1-main\\papers\", title + \".pdf\") \n",
    "            self.download_paper_pdf(pdf_link, local_pdf_path)\n",
    "            authors = [author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')]\n",
    "            published = entry.find('{http://www.w3.org/2005/Atom}published').text\n",
    "\n",
    "            papers.append({\n",
    "                \"title\": title,\n",
    "                \"summary\": summary,\n",
    "                \"link\": link,\n",
    "                \"pdf_link\": local_pdf_path,\n",
    "                \"authors\": authors,\n",
    "                \"published\": published,\n",
    "                \"year\": published[:4]  # Extract the year from the published date\n",
    "            })\n",
    "\n",
    "        return papers\n",
    "\n",
    "\n",
    "# Neo4j Storage Tool\n",
    "\n",
    "class Neo4jResearchStore:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def store_paper_with_entities(self, paper):\n",
    "        with self.driver.session() as session:\n",
    "            session.write_transaction(self._create_paper_with_entities, paper)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_paper_with_entities(tx, paper):\n",
    "        # Create the Paper node\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (p:Paper {title: $title})\n",
    "            SET p.link = $link\n",
    "            \"\"\",\n",
    "            title=paper['title'],\n",
    "            link=paper['link']\n",
    "        )\n",
    "\n",
    "        # Create Summary node and relationship\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (s:Summary {text: $summary})\n",
    "            WITH s\n",
    "            MATCH (p:Paper {title: $title})\n",
    "            MERGE (p)-[:HAS_SUMMARY]->(s)\n",
    "            \"\"\",\n",
    "            summary=paper['summary'][:500],  # Optional: Limit summary length for storage\n",
    "            title=paper['title']\n",
    "        )\n",
    "\n",
    "        # Create Year node and relationship\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (y:Year {year: $year})\n",
    "            WITH y\n",
    "            MATCH (p:Paper {title: $title})\n",
    "            MERGE (p)-[:PUBLISHED_IN]->(y)\n",
    "            \"\"\",\n",
    "            year=paper['year'],\n",
    "            title=paper['title']\n",
    "        )\n",
    "\n",
    "        # Create Author nodes and relationships\n",
    "        for author_name in paper['authors']:\n",
    "            tx.run(\"\"\"\n",
    "                MERGE (a:Author {name: $author_name})\n",
    "                WITH a\n",
    "                MATCH (p:Paper {title: $title})\n",
    "                MERGE (a)-[:AUTHORED]->(p)\n",
    "                \"\"\",\n",
    "                author_name=author_name,\n",
    "                title=paper['title']\n",
    "            )\n",
    "\n",
    "    def store_paper_chunks(self, title, chunks, embeddings):\n",
    "        # Store each chunk as a node with a relationship to the paper\n",
    "        with self.driver.session() as session:\n",
    "            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                session.write_transaction(self._create_chunk_node, title, chunk, embedding, i)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_chunk_node(tx, title, chunk, embedding, chunk_id):\n",
    "        # Create Chunk node and connect it to the Paper node\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (p:Paper {title: $title})\n",
    "            CREATE (c:Chunk {text: $chunk, embedding: $embedding, chunk_id: $chunk_id})\n",
    "            MERGE (p)-[:HAS_CHUNK]->(c)\n",
    "            \"\"\",\n",
    "            title=title,\n",
    "            chunk=chunk,\n",
    "            embedding=embedding.tolist(),  # Convert embedding to a list for Neo4j\n",
    "            chunk_id=chunk_id\n",
    "        )\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text() or \"\"  # Adds page text if present\n",
    "    return text\n",
    "\n",
    "def create_chunks_and_embeddings(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)  # Adjust size as needed\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    # Step 1: Split text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    # Step 2: Generate embeddings for each chunk\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "    return chunks, embeddings\n",
    "\n",
    "\n",
    "# Main function to fetch papers and store in Neo4j\n",
    "def fetch_and_store_arxiv_papers(topic, max_results, neo4j_uri, neo4j_user, neo4j_password):\n",
    "    # Fetch papers\n",
    "    arxiv_tool = ArxivResearchTool(max_results=max_results)\n",
    "    papers = arxiv_tool.fetch_papers(topic)\n",
    "\n",
    "    neo4j_store = Neo4jResearchStore(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "    for paper in papers:\n",
    "        try:\n",
    "            title = paper[\"title\"]\n",
    "            text = extract_text_from_pdf(paper[\"pdf_link\"])\n",
    "            neo4j_store.store_paper_with_entities(paper)\n",
    "            chunks, embeddings = create_chunks_and_embeddings(text)\n",
    "            neo4j_store.store_paper_chunks(title, chunks, embeddings) \n",
    "        except:\n",
    "            continue   \n",
    "    \n",
    "    neo4j_store.close()\n",
    "    print(f\"Stored {len(papers)} papers on the topic '{topic}' in the Neo4j database.\")\n",
    "    return f\"Stored {len(papers)} papers on the topic '{topic}' in the Neo4j database.\"\n",
    "\n",
    "# Define the LangChain Tool that combines fetching and storing\n",
    "def arxiv_to_neo4j_tool_func(topic: str):\n",
    "    # Neo4j connection details\n",
    "    neo4j_uri = \"bolt://localhost:7687\"\n",
    "    neo4j_user = \"neo4j\"\n",
    "    neo4j_password = \"your_password\"\n",
    "\n",
    "    return fetch_and_store_arxiv_papers(topic, max_results=5, neo4j_uri=neo4j_uri, neo4j_user=neo4j_user, neo4j_password=neo4j_password)\n",
    "\n",
    "arxiv_neo4j_tool = Tool(\n",
    "    name=\"ArXiv to Neo4j Tool\",\n",
    "    func=arxiv_to_neo4j_tool_func,\n",
    "    description=\"Fetches research papers from ArXiv by topic and stores them in Neo4j.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Title)} {position: line: 1, column: 10, offset: 9} for query: \"MATCH (n:`Title`) WHERE n.embedding IS null AND any(k in $props WHERE n[k] IS NOT null) RETURN elementId(n) AS id, reduce(str='',k IN $props | str + '\\\\n' + k + ':' + coalesce(n[k], '')) AS text LIMIT 1000\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",      \n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    ")\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embeddings,\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",              \n",
    "    password=\"your_password\",\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Title\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "vector_retriever = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound nouns: ['advancements', 'Gen AI', 'AI', 'microsoft']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from keybert import KeyBERT\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_topics(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'PROPN']:\n",
    "            # If it's part of a compound noun, get the full phrase\n",
    "            if token.dep_ == 'compound':\n",
    "                compound = token.text + ' ' + token.head.text\n",
    "                nouns.append(compound)\n",
    "            # If it's not part of a compound already added\n",
    "            elif token.dep_ != 'compound':\n",
    "                nouns.append(token.text)\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "text = \"Give the latest advancements in Gen AI by microsoft in 2021\"\n",
    "\n",
    "compound_nouns = get_compound_nouns(text)\n",
    "print(\"Compound nouns:\", compound_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021']\n",
      "Identified topics: ['2021']\n",
      "['optimization problems', 'neural networks', 'researchers', 'learning algorithms', '\\n integration of quantum computing with machine learning algorithms', 'mechanics principles', 'field of computational biology', 'quantum principles', 'major breakthroughs in protein']\n",
      "\n",
      "Complex text topics: ['optimization problems', 'neural networks', 'researchers', 'learning algorithms', '\\n integration of quantum computing with machine learning algorithms', 'mechanics principles', 'field of computational biology', 'quantum principles', 'major breakthroughs in protein']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from keybert import KeyBERT\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# kw_model = KeyBERT()\n",
    "\n",
    "def extract_topics(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Identify potential topics by looking for:\n",
    "    # 1. Subject noun phrases (using dependency parsing)\n",
    "    # 2. Noun phrases that are objects of verbs like \"discuss\", \"cover\", \"explain\"\n",
    "    # 3. Noun compounds and their modifiers\n",
    "    \n",
    "    topics = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        # Find subject phrases\n",
    "        subjects = [tok for tok in sent \n",
    "                   if tok.dep_ in ('nsubj', 'nsubjpass') \n",
    "                   and not tok.is_stop]\n",
    "        \n",
    "        for subject in subjects:\n",
    "            # Get the full noun phrase containing the subject\n",
    "            phrase = ' '.join([tok.text for tok in subject.subtree \n",
    "                             if not tok.dep_ in ('punct', 'det')])\n",
    "            if phrase:\n",
    "                topics.append(phrase.lower())\n",
    "        \n",
    "        # Find noun compounds\n",
    "        for token in sent:\n",
    "            if token.dep_ == 'compound' and token.head.pos_ == 'NOUN':\n",
    "                phrase = ' '.join([token.text, token.head.text])\n",
    "                topics.append(phrase.lower())\n",
    "    \n",
    "    # Remove duplicates\n",
    "    topics = list(set(topics))\n",
    "\n",
    "    authors = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    titles = [ent.text for ent in doc.ents if ent.label_ == \"WORK_OF_ART\"]\n",
    "    years = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "    print(topics + authors + titles + years)\n",
    "    return topics + authors + titles + years\n",
    "\n",
    "# Test it\n",
    "text = \"\"\"\n",
    "Give me all papers published in Gen AI by microsoft in 2021\n",
    "\"\"\"\n",
    "\n",
    "topics = extract_topics(text)\n",
    "print(\"Identified topics:\", topics)\n",
    "\n",
    "# For more complex text:\n",
    "complex_text = \"\"\"\n",
    "The integration of quantum computing with machine learning algorithms presents new opportunities \n",
    "for solving complex optimization problems. Researchers are exploring how neural networks can \n",
    "benefit from quantum mechanics principles. The field of computational biology is also seeing \n",
    "major breakthroughs in protein folding prediction.\n",
    "\"\"\"\n",
    "\n",
    "complex_topics = extract_topics(complex_text)\n",
    "print(\"\\nComplex text topics:\", complex_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.tokens import Span\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "# patterns = [{\"label\": \"TOPIC\", \"pattern\": \"machine learning\"}, {\"label\": \"TOPIC\", \"pattern\": \"artificial intelligence\"}]\n",
    "# ruler.add_patterns(patterns)\n",
    "\n",
    "# # Test with a sentence\n",
    "# doc = nlp(\"Find papers on machine learning from 2021.\")\n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.label_)\n",
    "\n",
    "\n",
    "# def extract_entities_for_research_query(question):\n",
    "#     doc = nlp(question)\n",
    "\n",
    "#     authors = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "#     titles = [ent.text for ent in doc.ents if ent.label_ == \"WORK_OF_ART\"]\n",
    "#     years = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "#     topics = [ent.text for ent in doc.ents if ent.label_ == \"TOPIC\"]\n",
    "\n",
    "#     return {\"authors\": authors, \"titles\": titles, \"years\": years, \"topics\": topics}\n",
    "#     # return authors + titles + years + topics\n",
    "\n",
    "def extract_papers_from_topics(topics):\n",
    "    print('hi1')\n",
    "    list_of_papers = []\n",
    "    for topic in topics:\n",
    "        arxiv_to_neo4j_tool_func(topic)\n",
    "        list_of_papers.append(ArxivResearchTool.fetch_papers(topic))\n",
    "    \n",
    "    return list_of_papers\n",
    "    \n",
    "def full_extractor(question):\n",
    "    \n",
    "    topics = extract_topics(question)\n",
    "    print('hi2')\n",
    "    return extract_papers_from_topics(topics)\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "question = \"Give me all papers published in deep learning in 2021\"\n",
    "# print(extract_entities_for_research_query(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a test topic to fetch and store papers\n",
    "# test_topics = \n",
    "\n",
    "# # Run the tool function\n",
    "# output_message = arxiv_to_neo4j_tool_func(test_topic)\n",
    "\n",
    "# print(output_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    if not words:\n",
    "        return \"\"\n",
    "    full_text_query = \" AND \".join([f\"{word}~2\" for word in words])\n",
    "    print(f\"Generated Query: {full_text_query}\")\n",
    "    return full_text_query.strip()\n",
    "\n",
    "\n",
    "# # Fulltext index query\n",
    "def graph_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities_dic = extract_entities_for_research_query(question)\n",
    "    entities = entities_dic[\"authors\"] + entities_dic[\"titles\"] + entities_dic[\"years\"] + entities_dic[\"topics\"]\n",
    "    for entity in entities:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": entity},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_retriever(question: str):\n",
    "    graph_data = graph_retriever(question)\n",
    "    vector_data = [el.page_content for el in vector_retriever.invoke(question)]\n",
    "    final_data = f\"\"\"Graph data: {graph_data} \\n\n",
    "    vector data: {\"#Title \". join(vector_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\vishn\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "token=\"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\"\n",
    "from huggingface_hub import login\n",
    "login(token = token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_32200\\791270976.py:3: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  hf=HuggingFaceHub(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"HelloHelp\\nI'm a large language model, so I can understand and respond to a wide range of questions and topics. I can provide information, answer questions, and even engage in conversation. I'm here to help with any questions or topics you'd like to discuss. What's on your mind?\\n\\nWould you like to:\\n1. Ask a question on a specific topic?\\n2. Discuss a particular subject or issue?\\n3. Play a game or have some fun?\\n4. Get recommendations or suggestions?\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":-1},\n",
    "    huggingfacehub_api_token = token\n",
    ")\n",
    "text = (\"Hello\"\n",
    "       )\n",
    "prompt = \"Help\"\n",
    "query= text + prompt\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import  RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "question_answer_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "question_answer_prompt = ChatPromptTemplate.from_template(question_answer_template)\n",
    "\n",
    "question_answer_chain = (\n",
    "        {\n",
    "            \"context\": full_retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    | question_answer_prompt\n",
    "    | hf\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_retrieval_template = \"\"\"Present papers on the topic asked by the user in a nice format with year, author and topic. The list of papers available with you are:\n",
    "{context}\n",
    "\n",
    "prompt: {prompt}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "\n",
    "paper_retrieval_prompt = ChatPromptTemplate.from_template(paper_retrieval_template)\n",
    "\n",
    "paper_retrieval_chain = (\n",
    "    {\n",
    "        \"context\" : full_extractor,\n",
    "        \"prompt\" : RunnablePassthrough()\n",
    "    }\n",
    "    | paper_retrieval_prompt\n",
    "    | hf\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['papers published in', '2021']\n",
      "hi2\n",
      "hi1\n",
      "Paper downloaded successfully to D:\\Coding\\GraphRAG-with-Llama-3.1-main\\papers\\The Evolving Ecosystem of Predatory Journals: A Case Study in Indian  Perspective.pdf\n",
      "Paper downloaded successfully to D:\\Coding\\GraphRAG-with-Llama-3.1-main\\papers\\Publication and collaboration anomalies in academic papers originating  from a paper mill: evidence from a Russia-based paper mill.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_32200\\1490893056.py:71: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(self._create_paper_with_entities, paper)\n"
     ]
    }
   ],
   "source": [
    "result2 = paper_retrieval_chain.invoke(input=\"Give me all papers published in generative ai by microsoft in 2021\")\n",
    "#print(full_extractor(\"What is optimal control?\"))\n",
    "# res_start2 = \"Use natural language and be concise.\\nAnswer:\"\n",
    "# res_index2 = result2.find(res_start2)\n",
    "# # second_index = result['result'].find(res_start, res_index + len(res_start))\n",
    "# result_start2 = res_index2 + len(res_start2)\n",
    "# print(result2[result_start2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Present papers on the topic asked by the user in a nice format with year, author and topic. The list of papers available with you are:\n",
      "None\n",
      "\n",
      "prompt: Give me all papers published in deep learning in 2021\n",
      "Use natural language and be concise.\n",
      "Answer: I'd be happy to help you with that! Unfortunately, I don't have a comprehensive list of papers on deep learning from 2021. However, I can suggest some popular and influential papers in the field that were published in 2021. Here are a few:\n",
      "\n",
      "1. **\"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville** (2021) - This book is a comprehensive introduction to deep learning, covering topics such as neural networks,\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
